env_args:
    # env: 'TicTacToe'
    # env: 'Geister'
    # env: 'HungryGeese'
    # env: 'handyrl.envs.parallel_tictactoe'  # specify by path
    env: 'simpletask'
    param:
        depth: 6 # 深度(変更可能)
        hyperplane_n: 2 # 超平面次元数(変更可能)
        treasure: [[5, 2, 2]] # 報酬の場所(変更可能) #0番目は必ず(深度-1)になるように
        set_reward: [1] # 報酬の値(変更可能)
        other_reward: 0 # treasure以外に到達した時の報酬設定
        start_random: True # 初期地点をランダムにするか固定にするか / True: ランダムにする, False: 固定する
        pomdp_setting: # 途中報酬への対応(POMDP)
            pom_bool: False # 途中報酬を導入するか否か
            pom_state: [1, 1] # 途中報酬の座標(ここを通らないと報酬が得られない)
        random_trasures_setting: # 報酬の場所をランダムに設定
            random_trasures_bool: False # 報酬の場所をランダムに設定するか / True: 設定する, False: 設定しない
            random_trasures_num: 1 # 報酬の場所の個数
        random_reward_setting: # 確率的な報酬設定 ※現在はPOMDPとの併用不可
            random_reward_bool: False # 確率的な報酬で設定するか / True: 設定する, False: 設定しない
            random_reward: [[1], [1, 100]] # 報酬の量, 各要素は報酬の位置に対応
            random_reward_p: [[1], [0.9, 0.1]] # 報酬の確率 , 各要素の合計値は1になるように設定
        uns_setting:
            uns_bool: False # 非定常の導入するか否か
            uns_num: 1000 # 非定常の周期（報酬位置をどの程度の間隔で変更するか）
        observation_noise: 0 # 観測時にノイズがあるか（ 0:なし, 1:ノイズ小, 2;ノイズ大）
        jyotai_boolkari: True # ランダムな状態量の有無


train_args:
    default_learning_rate: 3.0e-8 # default: 3.0e-8 
    turn_based_training: False
    observation: False
    return_buckup: True # QL or AC or SAC => True, PG => False
    gamma: 1.0
    forward_steps: 2 # QL => 2
    burn_in_steps: 0  # for RNNs
    compress_steps: 4
    entropy_regularization: 0.001
    entropy_regularization_decay: 1.0
    target_model:
        use: True # It is not essential in distributed reinforcement learning.
        update: 'soft' # 'soft', 'hard'
        update_param: 0.001
    update_episodes: 500
    batch_size: 128 # 256
    minimum_episodes: 10000
    maximum_episodes: 10000
    epochs: 10000 # 最低でも15以上である必要がある
    saving_interval_epochs: 50
    num_batchers: 1
    eval_rate: 0.1
    worker:
        num_parallel: 1
    lambda: 0.0 # TD-Q-HARDMAX => 0.0
    policy_target: 'TD-Q-HARDMAX' # 'UPGO' 'VTRACE' 'TD' 'TD-Q' 'TD-Q-HARDMAX' 'MC'
    value_target: 'TD-Q-HARDMAX' # 'UPGO' 'VTRACE' 'TD' 'TD-Q' 'TD-Q-HARDMAX' 'MC'
    eval:
        opponent: ['random']
    seed: 0
    restart_epoch: 0
    agent:
        type: 'QL' # 'BASE' 'QL' 'RSRS',
        use_RND: True
        meta_policy: 'e-greedy' # 'e-greedy' 'softmax'
        mp_param: [0.8] # [0.1]
        ASC_type: 'VQ-SeTranVAE' # False, 'SeTranVAE', 'VQ-SeTranVAE'
        ASC_trajectory_length: 5 # ASC_trajectory_length = 0 is not use, ASC_trajectory_length > 0 
        ASC_mask_probabirity: 0.2 #  集合要素 2 以上の場合の 2 つめ以降の mask 率 TODO: 現在実装されていない
        ASC_dropout: 0.2 # ASC model の dropout 率
    contrastive_learning: 
        use: True
        temperature: 0.5
    loss_factor:
        rl: 1.0 # default: 1.0 # RL loss weight
        rnd: 1.0 # default: 1.0 # RND loss weight
        recon: 1.0 # default: 1.0 # reconstruction loss weight for VAE and VQ-VAE
        vae_kl: 1.0 # default: 1.0 # KL loss weight for VAE
        codebook: 1.0 # default: 1.0 # reconstruction loss for VQ-VAE
        commitment: 0.25 # default: 0.25 # commitment weight for VQ-VAE
        contrast: 1.0 # default: 1.0  # commitment weight for contrastive learning
        recon_p_set: 1.0 # default: 1.0 # policy reconstruction loss weight for Transformer-VAE
        recon_o_set: 1.0 # default: 1.0 # re_observation reconstruction loss weight for Transformer-VAE
        cos_weighted: 0.01 # default: 0.1 # re_observation reconstruction cos_weighted loss weight for Transformer-VAE
        hungarian: 1.0 # default: 0.8 # re_observation reconstruction hungarian loss weight for Transformer-VAE
    metadata:
        name: []
        # name: ['rnd_weight'] # Unimplemented
        # name: ['knn', 'global_aleph', 'regional_weight', 'global_return_size']
        # knn:
        #     size: 5000
        #     k: 32
        # regional_weight: 0.5
        # global_aleph: 1.0
        # global_return_size: 100
        # rnd_weight: 2.0 # Unimplemented

worker_args:
    server_address: '192.168.2.24'
    num_parallel: 128
