env_args:
    # env: 'TicTacToe'
    # env: 'Geister'
    # env: 'HungryGeese'
    # env: 'handyrl.envs.parallel_tictactoe'  # specify by path
    env: 'simple_pyramid'
    param:
        depth: 6 # 深度(変更可能)
        hyperplane_dim: 2 # 超平面次元数(変更可能)
        general_seed: -1 # 初期状態や報酬に関する乱数発生器の seed 
        start_random: True # 初期地点をランダムにするか固定にするか / True: ランダムにする, False: 固定する
        features:
            seed: -1 # 特徴量関係の乱数 seed（保存対象）
            dim: 2 # 0 だと特徴量が座標そのものになる, 1 以上なら任意の次元数の座標とは異なる特徴量が設定される
            obs_var: 0.01 # 状態特徴の観測時に乗るノイズ（標準正規分布の分散）
        rewards: 
            depth: [6] # 報酬の深度(変更可能)，深度は内部で 1 から数える, depth と同値なら終端報酬 list 形式で複数設置可能
            coordinates: [[1, 1]] # 報酬の超平面座標(変更可能), () ならランダムな超平面座標に設置
            type: ['fix'] # 報酬関数の種類, binominal（二項分布）/ normal（正規分布）/ fix（固定値）
            mu: [1.0] # 報酬平均, binominal（二項分布）では報酬確率 / normal（正規分布）では平均 / fix（固定値）では固定値
            var: [0.0] # 報酬分散, normal（正規分布）では分散 / 他は使わない
        keys: # POMDP 関係の設定, 報酬源が一つの時に適切に機能
            depth: [] # 報酬の鍵の深度(変更可能), [] なら POMDP は使わない, 深度は内部で 1 から数える, depth と同値なら終端報酬 list 形式で複数設置可能
            coordinates: [] # 報酬の鍵の超平面座標(変更可能), () ならランダムな超平面座標に設置
        shift: # 環境の変異設定（非定常にできるのは座標ではなく特徴量を別設定している時のみ）
            type: 'none' # 環境の変異パターン, none（しない）/ reset（完全変更）/ linear (線形変換)
            intercept: 0.0 # 環境変異の特徴量変換の際の切片
            slope: 1.0 # 環境変異の特徴量変換の際の傾き
            interval_episodes: -1 # 環境変異の間隔


train_args:
    default_learning_rate: 3.0e-8 # default: 3.0e-8 
    turn_based_training: False
    observation: False
    return_buckup: True # QL or AC or SAC => True, PG => False
    gamma: 1.0
    forward_steps: 2 # QL => 2
    burn_in_steps: 0  # for RNNs
    compress_steps: 4
    entropy_regularization: 0.001
    entropy_regularization_decay: 1.0
    target_model:
        use: True # It is not essential in distributed reinforcement learning.
        update: 'soft' # 'soft', 'hard'
        update_param: 0.001
    update_episodes: 500
    batch_size: 128 # 256
    minimum_episodes: 10000
    maximum_episodes: 20000
    epochs: 10000
    saving_interval_epochs: 50
    num_batchers: 1
    eval_rate: 0.1
    saving_env_status_interval_episodes: 10000
    worker:
        num_parallel: 1
    lambda: 0.0 # TD-Q-HARDMAX => 0.0
    policy_target: 'TD-Q-HARDMAX' # 'UPGO' 'VTRACE' 'TD' 'TD-Q' 'TD-Q-HARDMAX' 'MC'
    value_target: 'TD-Q-HARDMAX' # 'UPGO' 'VTRACE' 'TD' 'TD-Q' 'TD-Q-HARDMAX' 'MC'
    eval:
        opponent: ['random']
    seed: 0
    restart_epoch: 0
    agent:
        type: 'ASC' # 'BASE' 'QL' 'RSRS' 'ASC'
        use_RND: True
        meta_policy: 'e-greedy' # 'e-greedy' 'softmax'
        mp_param: [0.8] # [0.1]
        subtype: 'QL' # 'BASE' 'QL' 'RSRS'
        play_subagent_base_prob: 1.0 # サブエージェントが軌跡を生成する確率の初期値
        play_subagent_lower_prob: 0.5 # サブエージェントが軌跡を生成する確率の下限
        play_subagent_decay_per_ep: 0.000001 # サブエージェントが軌跡を生成する確率のエピソードごとの減少値
        ASC_type: 'VQ-SeTranVAE' # '', 'VAE', 'VQ-VAE', 'SeTranVAE', 'VQ-SeTranVAE'
        ASC_trajectory_length: 5 # ASC_trajectory_length = 0 is not use, ASC_trajectory_length > 0 
        ASC_mask_probabirity: 0.2 #  集合要素 2 以上の場合の 2 つめ以降の mask 率 TODO: 現在実装されていない
        ASC_dropout: 0.2 # ASC model の dropout 率
    contrastive_learning: 
        use: True # Whether to perform contrastive learning
        temperature: 0.5 # The temperature parameter in contrastive learning
    loss_coefficient:
        rl: 1.0 # default: 1.0 # RL loss coefficient
        rnd: 1.0 # default: 1.0 # RND loss coefficient
        recon: 1.0 # default: 1.0 # reconstruction loss coefficient for VAE and VQ-VAE
        vae_kl: 1.0 # default: 1.0 # KL loss coefficient for VAE
        codebook: 1.0 # default: 1.0 # reconstruction loss coefficient for VQ-VAE
        commitment: 0.25 # default: 0.25 # commitment coefficient for VQ-VAE
        contrast: 1.0 # default: 1.0  # commitment coefficient for contrastive learning
        recon_p_set: 1.0 # default: 1.0 # policy reconstruction loss coefficient for SeTranVAE and VQ-SeTranVAE
        recon_o_set: 1.0 # default: 1.0 # re_observation reconstruction loss coefficient for SeTranVAE and VQ-SeTranVAE
        cos_weighted: 0.01 # default: 0.1 # re_observation reconstruction cos_weighted loss coefficient for SeTranVAE and VQ-SeTranVAE
        hungarian: 1.0 # default: 0.8 # re_observation reconstruction hungarian loss coefficient for SeTranVAE and VQ-SeTranVAE
    metadata:
        name: []
        # name: ['rnd_weight'] # Unimplemented
        # name: ['knn', 'global_aleph', 'regional_weight', 'global_return_size']
        # knn:
        #     size: 5000
        #     k: 32
        # regional_weight: 0.5
        # global_aleph: 1.0
        # global_return_size: 100
        # rnd_weight: 2.0 # Unimplemented

worker_args:
    server_address: '192.168.2.24'
    num_parallel: 128
